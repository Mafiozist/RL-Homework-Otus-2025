{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3efa70a0",
   "metadata": {},
   "source": [
    "# Занятие 2. Базовые понятия\n",
    "---\n",
    "\n",
    "[Демо код тут](./docs/rl-02/demo.ipynb) <br/>\n",
    "[Концпект лекций](./docs/конспект%20лекций%20RL.pdf)\n",
    "\n",
    "## Базовые понятия\n",
    "\n",
    "Пример игры для ввода в RL \n",
    "\n",
    "![Игра](./docs/rl-02/1.png)\n",
    "\n",
    "Симметричная стратегия для победы\n",
    "\n",
    "![Стратегия победы](./docs/rl-02/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597c4dd",
   "metadata": {},
   "source": [
    "# В чем суть метода RL\n",
    "\n",
    "По предыдущей задаче, подход в дискретном программировании, а суть RL в методе проб и ошибок\n",
    "\n",
    "![Суть RL](./docs/rl-02/3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921329fa",
   "metadata": {},
   "source": [
    "## Постановка задачи как RL\n",
    "\n",
    "Необходимо выделить среду и агента. Агент в данном случае тот, кто есть пельмени, а среда это обратная связь которая показывает состояние среды после дествий агента. \n",
    "\n",
    "![Суть RL](./docs/rl-02/4.png)\n",
    "\n",
    "Среда состоит из набора состояний (**S** - пространство состояний *кол-во пельменей перед каждым ходом*, **A** - пространство действия агента, **P** - функция переходов из одного состояния в другое). Сами ходы напрямую зависят от состояний. Для каждого хода действия должны быть четко определены иначе задача плохо структурирована и не подходит для RL.\n",
    "\n",
    "![Пространство действий](./docs/rl-02/5.png)\n",
    "\n",
    "В общем виде среда может быть *стахостичной* и вероятности задаются в формате числа с плавающей точкой. (*-* - это состояние в рамках задачи, а *+* переход). Когда для задачу можно описать в виде графа - такая среда называется **табличной**. \n",
    "\n",
    "![Действия](./docs/rl-02/6.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a55c9e",
   "metadata": {},
   "source": [
    "## Модель мира\n",
    "\n",
    "Среда на каждое действие должна дать измеримую обратную связь. (награды формировались из теории управления были штрафы и награды, в зависимости от страны которая развивались). В процессе игры иногда сложно выдать награду на шаги и обычно дают награду 0, а если в конце все отлично - дают положительную велечину, но и есть другой подход при создании такого варианта наград, когда выдаем награду по мат. ожиданию после каждого хода. \n",
    "\n",
    "![Награды](./docs/rl-02/7.png)\n",
    "\n",
    "![Переход к марковскому процессу принятия решения](./docs/rl-02/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030652ea",
   "metadata": {},
   "source": [
    "## Представление в виде графа\n",
    "\n",
    "Зеленые кружки - состояние, а красные переходы.\n",
    "\n",
    "![Граф представления](./docs/rl-02/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e4cab",
   "metadata": {},
   "source": [
    "## Политика\n",
    "\n",
    "Политика является результатам проектирования RL.\n",
    "\n",
    "![Представление ](./docs/rl-02/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b729115",
   "metadata": {},
   "source": [
    "# Занятие 3. Основные алгоритмы\n",
    "\n",
    "[Презентация](./docs/rl-03/presentation.pdf)  \n",
    "[Лекции](./docs/конспект%20лекций%20RL.pdf)\n",
    "\n",
    "- v-функция и q-функция\n",
    "- уравнение Беллмана\n",
    "\n",
    "## Описание игры в терминах MDP\n",
    "\n",
    "![Игра](./docs/rl-03/1.png)\n",
    "\n",
    "1) Сперва нужно разобраться сколько будет состояний в этой игре.\n",
    "2) Необходимо описать все эти состояния с добавление вероятности. \n",
    "(Зеленый-состояния, красный - действия. Красная стрелка - наши действия, при наступлении нашего действия есть исходы - синие стрелки)\n",
    "\n",
    "![Описание состояний](./docs/rl-03/2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d8023a",
   "metadata": {},
   "source": [
    "## Value - функция\n",
    "\n",
    "Найти такую политику, где можно получить максимальную награду. Состояние, из которого нельзя выйти - **терминальное**.\n",
    "\n",
    "![Примитивная задача](./docs/rl-03/3.png)\n",
    "\n",
    "Какие политики тут могут быть? Политики можно определить как **детерминированные** переходы и **вероятностные**.\n",
    "\n",
    "![Политики](./docs/rl-03/4.png)\n",
    "\n",
    "Какое же алгоритм будет для политики 4? Основная проблема в том, что неизвестно точно куда мы попадем. В таких случаях помогает мат. ожидание. \n",
    "\n",
    "![Простое объяснение мат. ожидания](./docs/rl-03/5.png)  \n",
    "![Политики](./docs/rl-03/6.png)\n",
    "\n",
    "В таком случае иногда лучше решать задачу не в лоб начиная с A, а начиная с терминальных вершин. В результате, используя политику P4 мы получим ответ 7.6.\n",
    "\n",
    "![Ответ по политике p4](./docs/rl-03/7.png)\n",
    "\n",
    "Таким образом **value-функция** приминима только при условии наличия правил т.е. **политки** и соотвественно конкретное **состояние** среды. (Сколько можно заработать стартуя из данной вершини по данному состоянию)\n",
    "\n",
    "![value-функция схемма](./docs/rl-03/8.png)\n",
    "\n",
    "Под **жадной политикой** подразумевается та политика, где на каждом ходу выбирается лучший вариант. **Оптимальная политика** - политика под которой мы получаем наибольшую награду. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf04b99",
   "metadata": {},
   "source": [
    "## Оптимальная value-функция (v^*)\n",
    "\n",
    "v* считается для лучшей политик. В данном примере, для 3-х вершин уже все известно. Оптимальный вариант v* считается в зависимости от выбранной детерминированной или вероятностной политики.\n",
    "\n",
    "![оптимальная value-функция](./docs/rl-03/9.png)\n",
    "\n",
    "Если выбран первый вариант, то просто берется текущий узел и и масимальное значение от перехода между состояниями + значение v* в конечном состоянии.\n",
    "\n",
    "![нахождение значения оптимальной функции](./docs/rl-03/10.png)\n",
    "\n",
    "Отсюда получается, что мы не знаем **политику**, но знаем конечную награду. Так как же получить политику зная v*? Чтобы этого добиться необходимо сохранять еще 1 значение Q.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8698f",
   "metadata": {},
   "source": [
    "## Quality - функция (q-функция)\n",
    "\n",
    "(сколько мы сможем заработать очнувшись в состоянии **s** выбрав переход **a** и политику **pi**) \n",
    "\n",
    "![Пример нахождения q-функции](./docs/rl-03/11.png)\n",
    "\n",
    "Получается, что нахождение Q* награда которую мы получим в текущей вершине + награда которую мы получим сдвинвшись в вершину V*.\n",
    "\n",
    "![Пример нахождения q-функции](./docs/rl-03/12.png)\n",
    "![Пример нахождения q-функции](./docs/rl-03/13.png)\n",
    "\n",
    "При стахостическом переходе, все это записывается используя мат.ожидание.\n",
    "\n",
    "![Стахостический случая](./docs/rl-03/14.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0b1a3",
   "metadata": {},
   "source": [
    "## Циклы\n",
    "\n",
    "Такую задачу решить сложно поскольку лучшая политика стремится к бесконечности т.е. ряд расходящийся и нельзя применить сумму. Практическая область RL решает это используя формализацию используя коэффициент дисконтирования т.е. каждый шаг имеет вероятность гамма, что этот ход завершит процесс.\n",
    "\n",
    "![Циклы](./docs/rl-03/15.png)\n",
    "\n",
    "Тогда формула примет вид. Случаи с 0 и 1 не очень интересны поскольку либо ничего нет, либо его нет коэффициента при 1 и мы остаемся в цикле.  \n",
    "\n",
    "![коэф дисконтирования](./docs/rl-03/16.png)\n",
    "\n",
    "Теперь посчитаем value - функцию при новом коэффициентом дисконтирования и политике, которая зашла в цикл*.\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "V(E) = 1 + \\gamma V(B) \\\\\n",
    "V(B) = 1 + \\gamma V(E)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Получается выражая V(E)\n",
    "\n",
    "$$\n",
    " V(E) = 1 + \\gamma (1+ \\gamma V(E)) = 1 + \\gamma + \\gamma^2 V(E) \\\\\n",
    " V(E)(1 - \\gamma^2) = 1 + \\gamma \\\\\n",
    "$$\n",
    "\n",
    "После преобразований можем получить:\n",
    "\n",
    "$$\n",
    " V(E) = \\frac{1+\\gamma}{1-\\gamma^2} = \\frac{1}{1-\\gamma} \\\\\n",
    " V(B) = 1 + \\frac{\\gamma}{1-\\gamma}\n",
    "$$\n",
    "\n",
    "Если решать задачу по другому (так делать можно не всегда!). \n",
    "$$\n",
    "\\begin{cases}\n",
    " V(E) = 1 + \\gamma (1 + \\gamma (...)) = 1 + \\gamma + \\gamma^2 + ... \\gamma^n \\\\\n",
    " V(E) = \\gamma + \\gamma^2 + \\gamma^3 + ... + \\gamma^n \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Если отнять одно от другого:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    1 = \\gamma V(E) - V(E) \\\\\n",
    "    1 = V(E)(\\gamma - 1) \\\\\n",
    "    V(E) = 1 + \\frac{\\gamma}{1-\\gamma}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "По сути, способ решения уравнения описанный выше - является уравнением Бэлмана.\n",
    "\n",
    "![Уравнения бэлмана](./docs/rl-03/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c052c6df",
   "metadata": {},
   "source": [
    "## Value-iteration алгоритм -> подбираем V* и определяем политику\n",
    "\n",
    "Алгоритм занимается приближением V* т.е. делает множество маленьких шажков. Этот алгоритм считает v-функцию, но по этим действиям можно будет восстановить политику. \n",
    "\n",
    "Хорошее описание политики - [Клик](https://gibberblot.github.io/rl-notes/single-agent/value-iteration.html#)\n",
    "\n",
    "1) Инициализирум V функцию\n",
    "2) Пересчитываем V\n",
    "\n",
    "![Value-iteration](./docs/rl-03/18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f6e73",
   "metadata": {},
   "source": [
    "## Policy-iteration -> точно находит V* и приближает нахождение политики\n",
    "\n",
    "Является модификацией Value-iteration и в сравнении, делает меньше шагов, но больших, за счет того что на каждом шаге происход вычисления  **PolicyEvaluation**. В данном случае мы инициализируем не V, а Q.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl-homework)",
   "language": "python",
   "name": "rl-homework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
