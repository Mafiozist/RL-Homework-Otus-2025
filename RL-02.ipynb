{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3efa70a0",
   "metadata": {},
   "source": [
    "Все что успели сделать в рамках 2 занятия (1-ое вводное). \n",
    "Теория + что делали.\n",
    "---\n",
    "\n",
    "# Базовые понятия\n",
    "\n",
    "Пример игры для ввода в RL \n",
    "\n",
    "![Игра](./docs/rl-02/1.png)\n",
    "\n",
    "Симметричная стратегия для победы\n",
    "\n",
    "![Стратегия победы](./docs/rl-02/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597c4dd",
   "metadata": {},
   "source": [
    "# В чем суть метода RL\n",
    "\n",
    "По предыдущей задаче, подход в дискретном программировании, а суть RL в методе проб и ошибок\n",
    "\n",
    "![Суть RL](./docs/rl-02/3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921329fa",
   "metadata": {},
   "source": [
    "# Постановка задачи как RL\n",
    "\n",
    "Необходимо выделить среду и агента. Агент в данном случае тот, кто есть пильмени, а среда это обратная связь которая показывает состояние среды после дествий агента. \n",
    "\n",
    "![Суть RL](./docs/rl-02/4.png)\n",
    "\n",
    "Среда состоит из набора состояний (**S** - пространство состояний *кол-во пельменей перед каждым ходом*, **A** - пространство действия агента, **P** - функция переходов из одного состояния в другое). Сами ходы напрямую зависят от состояний. Для каждого хода действия должны быть четко определены иначе задача плохо структурирована и не подходит для RL.\n",
    "\n",
    "![Пространство действий](./docs/rl-02/5.png)\n",
    "\n",
    "В общем виде среда может быть *стахостичной* и вероятности задаются в формате числа с плавающей точкой. (*-* - это состояние в рамках задачи, а *+* переход). Когда для задачу можно описать в виде графа - такая среда называется **табличной**. \n",
    "\n",
    "![Действия](./docs/rl-02/6.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a55c9e",
   "metadata": {},
   "source": [
    "# Модель мира\n",
    "\n",
    "Среда на каждое действие должна дать измеримую обратную связь. (награды формировались из теории управления были штрафы и награды, в зависимости от страны которая развивались). В процессе игры иногда сложно выдать награду на шаги и обычно дают награду 0, а если в конце все отлично - дают положительную велечину, но и есть другой подход при создании такого варианта наград, когда выдаем награду по мат. ожиданию после каждого хода. \n",
    "\n",
    "![Награды](./docs/rl-02/7.png)\n",
    "\n",
    "![Переход к марковскому процессу принятия решения](./docs/rl-02/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030652ea",
   "metadata": {},
   "source": [
    "# Представление в виде графа\n",
    "\n",
    "Зеленые кружки - состояние, а красные переходы.\n",
    "\n",
    "![Граф представления](./docs/rl-02/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e4cab",
   "metadata": {},
   "source": [
    "# Политика\n",
    "\n",
    "Политика является результатам проектирования RL.\n",
    "\n",
    "![Представление ](./docs/rl-02/10.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl-homework)",
   "language": "python",
   "name": "rl-homework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
